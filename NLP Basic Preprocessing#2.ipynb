{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'D:\\#DATA Science\\NLP\\NLP\\IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Emojis in Text:\n",
    "\n",
    "When working with text data that includes emojis, you have a few options for handling them:\n",
    "\n",
    "1. **Removing Emojis**: You can remove emojis from the text to simplify the data and focus on the textual content itself.\n",
    "\n",
    "2. **Replacing Emojis**: Alternatively, you can replace emojis with specific text, which can help maintain the context and meaning of the original emoji.\n",
    "\n",
    "#### 1. Removing Emojis:\n",
    "\n",
    "To remove emojis, you can use regular expressions or other text processing methods to identify and remove the Unicode characters representing emojis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you? \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\"\n",
    "                               \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               \"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    clean_text = emoji_pattern.sub(r'', text)\n",
    "    return clean_text\n",
    "\n",
    "text = \"Hello!ðŸ˜ŠðŸš€ðŸŒ¸ How are you? ðŸ˜ŠðŸš€ðŸŒ¸\"\n",
    "cleaned_text = remove_emojis(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Replacing Emojis:\n",
    "\n",
    "You can replace emojis with text representations that capture the emotion or meaning of the emoji. This can help retain the sentiment or context of the original emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The emoji is saying:  :smiling_face_with_smiling_eyes:\n",
      "Babar Azam is on  ðŸ”¥  :smiling_face_with_smiling_eyes:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "text2 = 'ðŸ˜Š'\n",
    "no_emoji = emoji.demojize(text2)\n",
    "print(\"The emoji is saying: \",no_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Babar Azam is on :fire:.\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Babar Azam is on ðŸ”¥'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization in NLP:\n",
    "\n",
    "1. **Using `split()` Function**: This is a basic method where text is split based on spaces or other specified delimiters.\n",
    "\n",
    "2. **Using Regular Expression**: Regular expressions can be used to define more complex tokenization patterns, allowing you to capture words and other patterns.\n",
    "\n",
    "3. **Using NLTK Tokenizers**: NLTK (Natural Language Toolkit) provides powerful tokenization tools, such as `word_tokenize()` for word-level tokenization and `sent_tokenize()` for sentence-level tokenization.\n",
    "\n",
    "4. **Using spaCy**: spaCy is a popular NLP library that offers advanced tokenization capabilities along with other linguistic features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bismillah!',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Hamza',\n",
       " 'Ali,',\n",
       " 'I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'Pakistan.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'Student',\n",
       " 'at',\n",
       " 'U.E.T',\n",
       " 'Peshawar',\n",
       " 'and',\n",
       " 'Alhamdulillah!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_intro = \"Bismillah! My name is Hamza Ali, I am from Pakistan. I am a Student at U.E.T Peshawar and Alhamdulillah!\"\n",
    "my_intro.split()        # just using split for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'Pakistan', 'I', 'love', 'my', 'country']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "my_info = \"I am a Pakistan!, I love my country.!\"\n",
    "tokens = re.findall(\"[\\w']+\", my_info)              ### regular expression for tokenization\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'Pakistan', '!', ',', 'I', 'love', 'my', 'country', '.', '!']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "word_tokenize(my_info)                  ### tokenization using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bismillah!',\n",
       " 'My name is Hamza Ali, I am from Pakistan.',\n",
       " 'I am a Student at U.E.T Peshawar and Alhamdulillah!']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(my_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bismillah\n",
      "!\n",
      "My\n",
      "name\n",
      "is\n",
      "Hamza\n",
      "Ali\n",
      ",\n",
      "I\n",
      "am\n",
      "from\n",
      "Pakistan\n",
      ".\n",
      "I\n",
      "am\n",
      "a\n",
      "Student\n",
      "at\n",
      "U.E.T\n",
      "Peshawar\n",
      "and\n",
      "Alhamdulillah\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(my_intro)\n",
    "for tokens in doc:\n",
    "    print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using PorterStemmer in NLTK:\n",
    "\n",
    "Stemming is a text normalization technique that reduces words to their base or root form. The Porter stemming algorithm is available in NLTK and is a widely used method for this purpose.\n",
    "\n",
    "To use the PorterStemmer in NLTK:\n",
    "\n",
    "1. **Import NLTK and PorterStemmer:**\n",
    "\n",
    "   Before using the PorterStemmer, import the NLTK library and the PorterStemmer class. You'll also need to download necessary data using `nltk.download('punkt')`.\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk.stem import PorterStemmer\n",
    "   nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "PS = PorterStemmer()\n",
    "def stemmed_words(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([PS.stem(word) for word in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am move over the road and sing a song while chill over the late night'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_for_stemming = \"I am moving over the road and singing a song while chilling over the late night\"\n",
    "stemmed_words(word_for_stemming)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization in Natural Language Processing:\n",
    "\n",
    "Lemmatization is a text normalization technique that reduces words to their base or dictionary form, known as the lemma. Unlike stemming, which simply removes prefixes and suffixes to obtain a root form, lemmatization considers the context and part-of-speech (POS) of words to produce valid words.\n",
    "\n",
    "In lemmatization, words are transformed based on their part-of-speech tag, preserving grammatical accuracy.\n",
    "\n",
    "To perform lemmatization using NLTK:\n",
    "\n",
    "1. **Import NLTK and WordNetLemmatizer:**\n",
    "\n",
    "   Import the NLTK library and the WordNetLemmatizer class. You'll need to download necessary data using `nltk.download('punkt')`.\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk.stem import WordNetLemmatizer\n",
    "   nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'walking'\n",
    "Lemmatizer.lemmatize(word, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"My name is Hamza, I am living with my family, I am Studing IT and Engineering and eating pizza.\"\n",
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word               lemma              \n",
      "My                 My                 \n",
      "name               name               \n",
      "is                 be                 \n",
      "Hamza              Hamza              \n",
      "I                  I                  \n",
      "am                 be                 \n",
      "living             live               \n",
      "with               with               \n",
      "my                 my                 \n",
      "family             family             \n",
      "in                 in                 \n",
      "Mansehra           Mansehra           \n",
      "we                 we                 \n",
      "are                be                 \n",
      "living             live               \n",
      "happily            happily            \n",
      "and                and                \n",
      "eating             eat                \n",
      "dinner             dinner             \n"
     ]
    }
   ],
   "source": [
    "sentence = \"My name is Hamza, I am living with my family in Mansehra, we are living happily and eating dinner!\"\n",
    "punctuation = \"./!,?\"\n",
    "sentence_words = word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuation:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:19}{1:19}\".format(\"word\", \"lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:19}{1:19}\".format(word, Lemmatizer.lemmatize(word, pos='v')))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
